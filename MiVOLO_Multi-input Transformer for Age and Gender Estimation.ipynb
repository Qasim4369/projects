{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c912b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Generator, List, Optional, Tuple, Union, Any\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import torch\n",
    "from ultralytics.yolo.engine.results import Results\n",
    "from ultralytics.yolo.utils.plotting import Annotator, colors\n",
    "import os\n",
    "from ultralytics.yolo.engine.model import YOLO\n",
    "import logging\n",
    "import timm\n",
    "from ultralytics.yolo.engine.results import Results\n",
    "from timm.layers import set_layer_config\n",
    "from timm.models._factory import parse_model_name\n",
    "from timm.models._helpers import load_state_dict, remap_checkpoint\n",
    "from timm.models._hub import load_model_config_from_hf\n",
    "from timm.models._pretrained import PretrainedCfg, split_model_name_tag\n",
    "from timm.models._registry import is_model, model_entrypoint\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.layers import trunc_normal_\n",
    "from timm.models._builder import build_model_with_cfg\n",
    "from timm.models._registry import register_model\n",
    "from timm.models.volo import VOLO\n",
    "from timm.layers.bottleneck_attn import PosEmbedRel\n",
    "from timm.layers.helpers import make_divisible\n",
    "from timm.layers.mlp import Mlp\n",
    "from timm.layers.trace_utils import _assert\n",
    "from timm.layers.weight_init import trunc_normal_\n",
    "from timm.data import resolve_data_config\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "import argparse\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf98f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP_ROUND_RATE = 0.1\n",
    "MIN_PERSON_CROP_NONZERO = 0.5\n",
    "\n",
    "\n",
    "def aggregate_votes_winsorized(ages, max_age_dist=6):\n",
    "    # Replace any annotation that is more than a max_age_dist away from the median\n",
    "    # with the median + max_age_dist if higher or max_age_dist - max_age_dist if below\n",
    "    median = np.median(ages)\n",
    "    ages = np.clip(ages, median - max_age_dist, median + max_age_dist)\n",
    "    return np.mean(ages)\n",
    "\n",
    "\n",
    "def cropout_black_parts(img, tol=0.3):\n",
    "    # Create a binary mask of zero pixels\n",
    "    zero_pixels_mask = np.all(img == 0, axis=2)\n",
    "    # Calculate the threshold for zero pixels in rows and columns\n",
    "    threshold = img.shape[0] - img.shape[0] * tol\n",
    "    # Calculate row sums and column sums of zero pixels mask\n",
    "    row_sums = np.sum(zero_pixels_mask, axis=1)\n",
    "    col_sums = np.sum(zero_pixels_mask, axis=0)\n",
    "    # Find the first and last rows with zero pixel sums above the threshold\n",
    "    start_row = np.argmin(row_sums > threshold)\n",
    "    end_row = img.shape[0] - np.argmin(row_sums[::-1] > threshold)\n",
    "    # Find the first and last columns with zero pixel sums above the threshold\n",
    "    start_col = np.argmin(col_sums > threshold)\n",
    "    end_col = img.shape[1] - np.argmin(col_sums[::-1] > threshold)\n",
    "    # Crop the image\n",
    "    cropped_img = img[start_row:end_row, start_col:end_col, :]\n",
    "    area = cropped_img.shape[0] * cropped_img.shape[1]\n",
    "    area_orig = img.shape[0] * img.shape[1]\n",
    "    return cropped_img, area / area_orig\n",
    "\n",
    "\n",
    "def natural_key(string_):\n",
    "    \"\"\"See http://www.codinghorror.com/blog/archives/001018.html\"\"\"\n",
    "    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n",
    "\n",
    "\n",
    "def add_bool_arg(parser, name, default=False, help=\"\"):\n",
    "    dest_name = name.replace(\"-\", \"_\")\n",
    "    group = parser.add_mutually_exclusive_group(required=False)\n",
    "    group.add_argument(\"--\" + name, dest=dest_name, action=\"store_true\", help=help)\n",
    "    group.add_argument(\"--no-\" + name, dest=dest_name, action=\"store_false\", help=help)\n",
    "    parser.set_defaults(**{dest_name: default})\n",
    "\n",
    "\n",
    "def cumulative_score(pred_ages, gt_ages, L, tol=1e-6):\n",
    "    n = pred_ages.shape[0]\n",
    "    num_correct = torch.sum(torch.abs(pred_ages - gt_ages) <= L + tol)\n",
    "    cs_score = num_correct / n\n",
    "    return cs_score\n",
    "\n",
    "\n",
    "def cumulative_error(pred_ages, gt_ages, L, tol=1e-6):\n",
    "    n = pred_ages.shape[0]\n",
    "    num_correct = torch.sum(torch.abs(pred_ages - gt_ages) >= L + tol)\n",
    "    cs_score = num_correct / n\n",
    "    return cs_score\n",
    "\n",
    "\n",
    "class ParseKwargs(argparse.Action):\n",
    "    def __call__(self, parser, namespace, values, option_string=None):\n",
    "        kw = {}\n",
    "        for value in values:\n",
    "            key, value = value.split(\"=\")\n",
    "            try:\n",
    "                kw[key] = ast.literal_eval(value)\n",
    "            except ValueError:\n",
    "                kw[key] = str(value)  # fallback to string (avoid need to escape on command line)\n",
    "        setattr(namespace, self.dest, kw)\n",
    "\n",
    "\n",
    "def box_iou(box1, box2, over_second=False):\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    If over_second == True, return mean(intersection-over-union, (inter / area2))\n",
    "\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    def box_area(box):\n",
    "        # box = 4xn\n",
    "        return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "    area1 = box_area(box1.T)\n",
    "    area2 = box_area(box2.T)\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "\n",
    "    iou = inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n",
    "    if over_second:\n",
    "        return (inter / area2 + iou) / 2  # mean(inter / area2, iou)\n",
    "    else:\n",
    "        return iou\n",
    "\n",
    "\n",
    "def split_batch(bs: int, dev: int) -> Tuple[int, int]:\n",
    "    full_bs = (bs // dev) * dev\n",
    "    part_bs = bs - full_bs\n",
    "    return full_bs, part_bs\n",
    "\n",
    "\n",
    "def assign_faces(\n",
    "    persons_bboxes: List[torch.tensor], faces_bboxes: List[torch.tensor], iou_thresh: float = 0.0001\n",
    ") -> Tuple[List[Optional[int]], List[int]]:\n",
    "    \"\"\"\n",
    "    Assign person to each face if it is possible.\n",
    "    Return:\n",
    "        - assigned_faces List[Optional[int]]: mapping of face_ind to person_ind\n",
    "                                            ( assigned_faces[face_ind] = person_ind ). person_ind can be None\n",
    "        - unassigned_persons_inds List[int]: persons indexes without any assigned face\n",
    "    \"\"\"\n",
    "\n",
    "    assigned_faces: List[Optional[int]] = [None for _ in range(len(faces_bboxes))]\n",
    "    unassigned_persons_inds: List[int] = [p_ind for p_ind in range(len(persons_bboxes))]\n",
    "\n",
    "    if len(persons_bboxes) == 0 or len(faces_bboxes) == 0:\n",
    "        return assigned_faces, unassigned_persons_inds\n",
    "\n",
    "    cost_matrix = box_iou(torch.stack(persons_bboxes), torch.stack(faces_bboxes), over_second=True).cpu().numpy()\n",
    "    persons_indexes, face_indexes = [], []\n",
    "\n",
    "    if len(cost_matrix) > 0:\n",
    "        persons_indexes, face_indexes = linear_sum_assignment(cost_matrix, maximize=True)\n",
    "\n",
    "    matched_persons = set()\n",
    "    for person_idx, face_idx in zip(persons_indexes, face_indexes):\n",
    "        ciou = cost_matrix[person_idx][face_idx]\n",
    "        if ciou > iou_thresh:\n",
    "            if person_idx in matched_persons:\n",
    "                # Person can not be assigned twice, in reality this should not happen\n",
    "                continue\n",
    "            assigned_faces[face_idx] = person_idx\n",
    "            matched_persons.add(person_idx)\n",
    "\n",
    "    unassigned_persons_inds = [p_ind for p_ind in range(len(persons_bboxes)) if p_ind not in matched_persons]\n",
    "\n",
    "    return assigned_faces, unassigned_persons_inds\n",
    "\n",
    "\n",
    "def class_letterbox(im, new_shape=(640, 640), color=(0, 0, 0), scaleup=True):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    if im.shape[0] == new_shape[0] and im.shape[1] == new_shape[1]:\n",
    "        return im\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    # ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im\n",
    "\n",
    "\n",
    "def prepare_classification_images(\n",
    "    img_list: List[Optional[np.ndarray]],\n",
    "    target_size: int = 224,\n",
    "    mean=IMAGENET_DEFAULT_MEAN,\n",
    "    std=IMAGENET_DEFAULT_STD,\n",
    "    device=None,\n",
    ") -> torch.tensor:\n",
    "\n",
    "    prepared_images: List[torch.tensor] = []\n",
    "\n",
    "    for img in img_list:\n",
    "        if img is None:\n",
    "            img = torch.zeros((3, target_size, target_size), dtype=torch.float32)\n",
    "            img = F.normalize(img, mean=mean, std=std)\n",
    "            img = img.unsqueeze(0)\n",
    "            prepared_images.append(img)\n",
    "            continue\n",
    "        img = class_letterbox(img, new_shape=(target_size, target_size))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img = img / 255.0\n",
    "        img = (img - mean) / std\n",
    "        img = img.astype(dtype=np.float32)\n",
    "\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img = torch.from_numpy(img)\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "        prepared_images.append(img)\n",
    "\n",
    "    prepared_input = torch.concat(prepared_images)\n",
    "\n",
    "    if device:\n",
    "        prepared_input = prepared_input.to(device)\n",
    "\n",
    "    return prepared_input\n",
    "\n",
    "\n",
    "def IOU(bb1: Union[tuple, list], bb2: Union[tuple, list], norm_second_bbox: bool = False) -> float:\n",
    "    # expects [ymin, xmin, ymax, xmax], doesnt matter absolute or relative\n",
    "    assert bb1[1] < bb1[3]\n",
    "    assert bb1[0] < bb1[2]\n",
    "    assert bb2[1] < bb2[3]\n",
    "    assert bb2[0] < bb2[2]\n",
    "\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1[1], bb2[1])\n",
    "    y_top = max(bb1[0], bb2[0])\n",
    "    x_right = min(bb1[3], bb2[3])\n",
    "    y_bottom = min(bb1[2], bb2[2])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1[3] - bb1[1]) * (bb1[2] - bb1[0])\n",
    "    bb2_area = (bb2[3] - bb2[1]) * (bb2[2] - bb2[0])\n",
    "    if not norm_second_bbox:\n",
    "        # compute the intersection over union by taking the intersection\n",
    "        # area and dividing it by the sum of prediction + ground-truth\n",
    "        # areas - the interesection area\n",
    "        iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    else:\n",
    "        # for cases when we search if second bbox is inside first one\n",
    "        iou = intersection_area / float(bb2_area)\n",
    "\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.01\n",
    "\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abb68811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossBottleneckAttn(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_out=None,\n",
    "        feat_size=None,\n",
    "        stride=1,\n",
    "        num_heads=4,\n",
    "        dim_head=None,\n",
    "        qk_ratio=1.0,\n",
    "        qkv_bias=False,\n",
    "        scale_pos_embed=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert feat_size is not None, \"A concrete feature size matching expected input (H, W) is required\"\n",
    "        dim_out = dim_out or dim\n",
    "        assert dim_out % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_head_qk = dim_head or make_divisible(dim_out * qk_ratio, divisor=8) // num_heads\n",
    "        self.dim_head_v = dim_out // self.num_heads\n",
    "        self.dim_out_qk = num_heads * self.dim_head_qk\n",
    "        self.dim_out_v = num_heads * self.dim_head_v\n",
    "        self.scale = self.dim_head_qk**-0.5\n",
    "        self.scale_pos_embed = scale_pos_embed\n",
    "\n",
    "        self.qkv_f = nn.Conv2d(dim, self.dim_out_qk * 2 + self.dim_out_v, 1, bias=qkv_bias)\n",
    "        self.qkv_p = nn.Conv2d(dim, self.dim_out_qk * 2 + self.dim_out_v, 1, bias=qkv_bias)\n",
    "\n",
    "        # NOTE I'm only supporting relative pos embedding for now\n",
    "        self.pos_embed = PosEmbedRel(feat_size, dim_head=self.dim_head_qk, scale=self.scale)\n",
    "\n",
    "        self.norm = nn.LayerNorm([self.dim_out_v * 2, *feat_size])\n",
    "        mlp_ratio = 4\n",
    "        self.mlp = Mlp(\n",
    "            in_features=self.dim_out_v * 2,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            act_layer=nn.GELU,\n",
    "            out_features=dim_out,\n",
    "            drop=0,\n",
    "            use_conv=True,\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AvgPool2d(2, 2) if stride == 2 else nn.Identity()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        trunc_normal_(self.qkv_f.weight, std=self.qkv_f.weight.shape[1] ** -0.5)  # fan-in\n",
    "        trunc_normal_(self.qkv_p.weight, std=self.qkv_p.weight.shape[1] ** -0.5)  # fan-in\n",
    "        trunc_normal_(self.pos_embed.height_rel, std=self.scale)\n",
    "        trunc_normal_(self.pos_embed.width_rel, std=self.scale)\n",
    "\n",
    "    def get_qkv(self, x, qvk_conv):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        x = qvk_conv(x)  # B, (2 * dim_head_qk + dim_head_v) * num_heads, H, W\n",
    "\n",
    "        q, k, v = torch.split(x, [self.dim_out_qk, self.dim_out_qk, self.dim_out_v], dim=1)\n",
    "\n",
    "        q = q.reshape(B * self.num_heads, self.dim_head_qk, -1).transpose(-1, -2)\n",
    "        k = k.reshape(B * self.num_heads, self.dim_head_qk, -1)  # no transpose, for q @ k\n",
    "        v = v.reshape(B * self.num_heads, self.dim_head_v, -1).transpose(-1, -2)\n",
    "\n",
    "        return q, k, v\n",
    "\n",
    "    def apply_attn(self, q, k, v, B, H, W, dropout=None):\n",
    "        if self.scale_pos_embed:\n",
    "            attn = (q @ k + self.pos_embed(q)) * self.scale  # B * num_heads, H * W, H * W\n",
    "        else:\n",
    "            attn = (q @ k) * self.scale + self.pos_embed(q)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        if dropout:\n",
    "            attn = dropout(attn)\n",
    "\n",
    "        out = (attn @ v).transpose(-1, -2).reshape(B, self.dim_out_v, H, W)  # B, dim_out, H, W\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        dim = int(C / 2)\n",
    "        x1 = x[:, :dim, :, :]\n",
    "        x2 = x[:, dim:, :, :]\n",
    "\n",
    "        _assert(H == self.pos_embed.height, \"\")\n",
    "        _assert(W == self.pos_embed.width, \"\")\n",
    "\n",
    "        q_f, k_f, v_f = self.get_qkv(x1, self.qkv_f)\n",
    "        q_p, k_p, v_p = self.get_qkv(x2, self.qkv_p)\n",
    "\n",
    "        # person to face\n",
    "        out_f = self.apply_attn(q_f, k_p, v_p, B, H, W)\n",
    "        # face to person\n",
    "        out_p = self.apply_attn(q_p, k_f, v_f, B, H, W)\n",
    "\n",
    "        x_pf = torch.cat((out_f, out_p), dim=1)  # B, dim_out * 2, H, W\n",
    "        x_pf = self.norm(x_pf)\n",
    "        x_pf = self.mlp(x_pf)  # B, dim_out, H, W\n",
    "\n",
    "        out = self.pool(x_pf)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73543bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "__all__ = [\"MiVOLOModel\"]  # model_registry will add each entrypoint fn to this\n",
    "\n",
    "\n",
    "def _cfg(url=\"\", **kwargs):\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"num_classes\": 1000,\n",
    "        \"input_size\": (3, 224, 224),\n",
    "        \"pool_size\": None,\n",
    "        \"crop_pct\": 0.96,\n",
    "        \"interpolation\": \"bicubic\",\n",
    "        \"fixed_input_size\": True,\n",
    "        \"mean\": IMAGENET_DEFAULT_MEAN,\n",
    "        \"std\": IMAGENET_DEFAULT_STD,\n",
    "        \"first_conv\": None,\n",
    "        \"classifier\": (\"head\", \"aux_head\"),\n",
    "        **kwargs,\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    \"mivolo_d1_224\": _cfg(\n",
    "        url=\"https://github.com/sail-sg/volo/releases/download/volo_1/d1_224_84.2.pth.tar\", crop_pct=0.96\n",
    "    ),\n",
    "    \"mivolo_d1_384\": _cfg(\n",
    "        url=\"https://github.com/sail-sg/volo/releases/download/volo_1/d1_384_85.2.pth.tar\",\n",
    "        crop_pct=1.0,\n",
    "        input_size=(3, 384, 384),\n",
    "    ),\n",
    "    \"mivolo_d2_224\": _cfg(\n",
    "        url=\"https://github.com/sail-sg/volo/releases/download/volo_1/d2_224_85.2.pth.tar\", crop_pct=0.96\n",
    "    ),\n",
    "    \"mivolo_d2_384\": _cfg(\n",
    "        url=\"https://github.com/sail-sg/volo/releases/download/volo_1/d2_384_86.0.pth.tar\",\n",
    "        crop_pct=1.0,\n",
    "        input_size=(3, 384, 384),\n",
    "    ),\n",
    "    \"mivolo_d3_224\": _cfg(\n",
    "        url=\"https://github.com/sail-sg/volo/releases/download/volo_1/d3_224_85.4.pth.tar\", crop_pct=0.96\n",
    "    ),\n",
    "    \"mivolo_d3_448\": _cfg(\n",
    "        url=\"https://github.com/sail-sg/volo/releases/download/volo_1/d3_448_86.3.pth.tar\",\n",
    "        crop_pct=1.0,\n",
    "        input_size=(3, 448, 448),\n",
    "    ),\n",
    "    \"mivolo_d4_224\": _cfg(\n",
    "        url=\"https://github.com/sail-sg/volo/releases/download/volo_1/d4_224_85.7.pth.tar\", crop_pct=0.96\n",
    "    ),\n",
    "    \"mivolo_d4_448\": _cfg(\n",
    "        url=\"https://github.com/sail-sg/volo/releases/download/volo_1/d4_448_86.79.pth.tar\",\n",
    "        crop_pct=1.15,\n",
    "        input_size=(3, 448, 448),\n",
    "    ),\n",
    "    \"mivolo_d5_224\": _cfg(\n",
    "        url=\"https://github.com/sail-sg/volo/releases/download/volo_1/d5_224_86.10.pth.tar\", crop_pct=0.96\n",
    "    ),\n",
    "    \"mivolo_d5_448\": _cfg(\n",
    "        url=\"https://github.com/sail-sg/volo/releases/download/volo_1/d5_448_87.0.pth.tar\",\n",
    "        crop_pct=1.15,\n",
    "        input_size=(3, 448, 448),\n",
    "    ),\n",
    "    \"mivolo_d5_512\": _cfg(\n",
    "        url=\"https://github.com/sail-sg/volo/releases/download/volo_1/d5_512_87.07.pth.tar\",\n",
    "        crop_pct=1.15,\n",
    "        input_size=(3, 512, 512),\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def get_output_size(input_shape, conv_layer):\n",
    "    padding = conv_layer.padding\n",
    "    dilation = conv_layer.dilation\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    stride = conv_layer.stride\n",
    "\n",
    "    output_size = [\n",
    "        ((input_shape[i] + 2 * padding[i] - dilation[i] * (kernel_size[i] - 1) - 1) // stride[i]) + 1 for i in range(2)\n",
    "    ]\n",
    "    return output_size\n",
    "\n",
    "\n",
    "def get_output_size_module(input_size, stem):\n",
    "    output_size = input_size\n",
    "\n",
    "    for module in stem:\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            output_size = [\n",
    "                (\n",
    "                    (output_size[i] + 2 * module.padding[i] - module.dilation[i] * (module.kernel_size[i] - 1) - 1)\n",
    "                    // module.stride[i]\n",
    "                )\n",
    "                + 1\n",
    "                for i in range(2)\n",
    "            ]\n",
    "\n",
    "    return output_size\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, img_size=224, stem_conv=False, stem_stride=1, patch_size=8, in_chans=3, hidden_dim=64, embed_dim=384\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert patch_size in [4, 8, 16]\n",
    "        assert in_chans in [3, 6]\n",
    "        self.with_persons_model = in_chans == 6\n",
    "        self.use_cross_attn = True\n",
    "\n",
    "        if stem_conv:\n",
    "            if not self.with_persons_model:\n",
    "                self.conv = self.create_stem(stem_stride, in_chans, hidden_dim)\n",
    "            else:\n",
    "                self.conv = True  # just to match interface\n",
    "                # split\n",
    "                self.conv1 = self.create_stem(stem_stride, 3, hidden_dim)\n",
    "                self.conv2 = self.create_stem(stem_stride, 3, hidden_dim)\n",
    "        else:\n",
    "            self.conv = None\n",
    "\n",
    "        if self.with_persons_model:\n",
    "\n",
    "            self.proj1 = nn.Conv2d(\n",
    "                hidden_dim, embed_dim, kernel_size=patch_size // stem_stride, stride=patch_size // stem_stride\n",
    "            )\n",
    "            self.proj2 = nn.Conv2d(\n",
    "                hidden_dim, embed_dim, kernel_size=patch_size // stem_stride, stride=patch_size // stem_stride\n",
    "            )\n",
    "\n",
    "            stem_out_shape = get_output_size_module((img_size, img_size), self.conv1)\n",
    "            self.proj_output_size = get_output_size(stem_out_shape, self.proj1)\n",
    "\n",
    "            self.map = CrossBottleneckAttn(embed_dim, dim_out=embed_dim, num_heads=1, feat_size=self.proj_output_size)\n",
    "\n",
    "        else:\n",
    "            self.proj = nn.Conv2d(\n",
    "                hidden_dim, embed_dim, kernel_size=patch_size // stem_stride, stride=patch_size // stem_stride\n",
    "            )\n",
    "\n",
    "        self.patch_dim = img_size // patch_size\n",
    "        self.num_patches = self.patch_dim**2\n",
    "\n",
    "    def create_stem(self, stem_stride, in_chans, hidden_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_chans, hidden_dim, kernel_size=7, stride=stem_stride, padding=3, bias=False),  # 112x112\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1, bias=False),  # 112x112\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1, bias=False),  # 112x112\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.conv is not None:\n",
    "            if self.with_persons_model:\n",
    "                x1 = x[:, :3]\n",
    "                x2 = x[:, 3:]\n",
    "\n",
    "                x1 = self.conv1(x1)\n",
    "                x1 = self.proj1(x1)\n",
    "\n",
    "                x2 = self.conv2(x2)\n",
    "                x2 = self.proj2(x2)\n",
    "\n",
    "                x = torch.cat([x1, x2], dim=1)\n",
    "                x = self.map(x)\n",
    "            else:\n",
    "                x = self.conv(x)\n",
    "                x = self.proj(x)  # B, C, H, W\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiVOLOModel(VOLO):\n",
    "    \"\"\"\n",
    "    Vision Outlooker, the main class of our model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers,\n",
    "        img_size=224,\n",
    "        in_chans=3,\n",
    "        num_classes=1000,\n",
    "        global_pool=\"token\",\n",
    "        patch_size=8,\n",
    "        stem_hidden_dim=64,\n",
    "        embed_dims=None,\n",
    "        num_heads=None,\n",
    "        downsamples=(True, False, False, False),\n",
    "        outlook_attention=(True, False, False, False),\n",
    "        mlp_ratio=3.0,\n",
    "        qkv_bias=False,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        post_layers=(\"ca\", \"ca\"),\n",
    "        use_aux_head=True,\n",
    "        use_mix_token=False,\n",
    "        pooling_scale=2,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            layers,\n",
    "            img_size,\n",
    "            in_chans,\n",
    "            num_classes,\n",
    "            global_pool,\n",
    "            patch_size,\n",
    "            stem_hidden_dim,\n",
    "            embed_dims,\n",
    "            num_heads,\n",
    "            downsamples,\n",
    "            outlook_attention,\n",
    "            mlp_ratio,\n",
    "            qkv_bias,\n",
    "            drop_rate,\n",
    "            attn_drop_rate,\n",
    "            drop_path_rate,\n",
    "            norm_layer,\n",
    "            post_layers,\n",
    "            use_aux_head,\n",
    "            use_mix_token,\n",
    "            pooling_scale,\n",
    "        )\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            stem_conv=True,\n",
    "            stem_stride=2,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            hidden_dim=stem_hidden_dim,\n",
    "            embed_dim=embed_dims[0],\n",
    "        )\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x).permute(0, 2, 3, 1)  # B,C,H,W-> B,H,W,C\n",
    "\n",
    "        # step2: tokens learning in the two stages\n",
    "        x = self.forward_tokens(x)\n",
    "\n",
    "        # step3: post network, apply class attention or not\n",
    "        if self.post_network is not None:\n",
    "            x = self.forward_cls(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x, pre_logits: bool = False, targets=None, epoch=None):\n",
    "        if self.global_pool == \"avg\":\n",
    "            out = x.mean(dim=1)\n",
    "        elif self.global_pool == \"token\":\n",
    "            out = x[:, 0]\n",
    "        else:\n",
    "            out = x\n",
    "        if pre_logits:\n",
    "            return out\n",
    "\n",
    "        features = out\n",
    "        fds_enabled = hasattr(self, \"_fds_forward\")\n",
    "        if fds_enabled:\n",
    "            features = self._fds_forward(features, targets, epoch)\n",
    "\n",
    "        out = self.head(features)\n",
    "        if self.aux_head is not None:\n",
    "            # generate classes in all feature tokens, see token labeling\n",
    "            aux = self.aux_head(x[:, 1:])\n",
    "            out = out + 0.5 * aux.max(1)[0]\n",
    "\n",
    "        return (out, features) if (fds_enabled and self.training) else out\n",
    "\n",
    "    def forward(self, x, targets=None, epoch=None):\n",
    "        \"\"\"simplified forward (without mix token training)\"\"\"\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_head(x, targets=targets, epoch=epoch)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _create_mivolo(variant, pretrained=False, **kwargs):\n",
    "    if kwargs.get(\"features_only\", None):\n",
    "        raise RuntimeError(\"features_only not implemented for Vision Transformer models.\")\n",
    "    return build_model_with_cfg(MiVOLOModel, variant, pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mivolo_d1_224(pretrained=False, **kwargs):\n",
    "    model_args = dict(layers=(4, 4, 8, 2), embed_dims=(192, 384, 384, 384), num_heads=(6, 12, 12, 12), **kwargs)\n",
    "    model = _create_mivolo(\"mivolo_d1_224\", pretrained=pretrained, **model_args)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mivolo_d1_384(pretrained=False, **kwargs):\n",
    "    model_args = dict(layers=(4, 4, 8, 2), embed_dims=(192, 384, 384, 384), num_heads=(6, 12, 12, 12), **kwargs)\n",
    "    model = _create_mivolo(\"mivolo_d1_384\", pretrained=pretrained, **model_args)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mivolo_d2_224(pretrained=False, **kwargs):\n",
    "    model_args = dict(layers=(6, 4, 10, 4), embed_dims=(256, 512, 512, 512), num_heads=(8, 16, 16, 16), **kwargs)\n",
    "    model = _create_mivolo(\"mivolo_d2_224\", pretrained=pretrained, **model_args)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mivolo_d2_384(pretrained=False, **kwargs):\n",
    "    model_args = dict(layers=(6, 4, 10, 4), embed_dims=(256, 512, 512, 512), num_heads=(8, 16, 16, 16), **kwargs)\n",
    "    model = _create_mivolo(\"mivolo_d2_384\", pretrained=pretrained, **model_args)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mivolo_d3_224(pretrained=False, **kwargs):\n",
    "    model_args = dict(layers=(8, 8, 16, 4), embed_dims=(256, 512, 512, 512), num_heads=(8, 16, 16, 16), **kwargs)\n",
    "    model = _create_mivolo(\"mivolo_d3_224\", pretrained=pretrained, **model_args)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mivolo_d3_448(pretrained=False, **kwargs):\n",
    "    model_args = dict(layers=(8, 8, 16, 4), embed_dims=(256, 512, 512, 512), num_heads=(8, 16, 16, 16), **kwargs)\n",
    "    model = _create_mivolo(\"mivolo_d3_448\", pretrained=pretrained, **model_args)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mivolo_d4_224(pretrained=False, **kwargs):\n",
    "    model_args = dict(layers=(8, 8, 16, 4), embed_dims=(384, 768, 768, 768), num_heads=(12, 16, 16, 16), **kwargs)\n",
    "    model = _create_mivolo(\"mivolo_d4_224\", pretrained=pretrained, **model_args)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mivolo_d4_448(pretrained=False, **kwargs):\n",
    "    \"\"\"VOLO-D4 model, Params: 193M\"\"\"\n",
    "    model_args = dict(layers=(8, 8, 16, 4), embed_dims=(384, 768, 768, 768), num_heads=(12, 16, 16, 16), **kwargs)\n",
    "    model = _create_mivolo(\"mivolo_d4_448\", pretrained=pretrained, **model_args)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mivolo_d5_224(pretrained=False, **kwargs):\n",
    "    model_args = dict(\n",
    "        layers=(12, 12, 20, 4),\n",
    "        embed_dims=(384, 768, 768, 768),\n",
    "        num_heads=(12, 16, 16, 16),\n",
    "        mlp_ratio=4,\n",
    "        stem_hidden_dim=128,\n",
    "        **kwargs\n",
    "    )\n",
    "    model = _create_mivolo(\"mivolo_d5_224\", pretrained=pretrained, **model_args)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mivolo_d5_448(pretrained=False, **kwargs):\n",
    "    model_args = dict(\n",
    "        layers=(12, 12, 20, 4),\n",
    "        embed_dims=(384, 768, 768, 768),\n",
    "        num_heads=(12, 16, 16, 16),\n",
    "        mlp_ratio=4,\n",
    "        stem_hidden_dim=128,\n",
    "        **kwargs\n",
    "    )\n",
    "    model = _create_mivolo(\"mivolo_d5_448\", pretrained=pretrained, **model_args)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def mivolo_d5_512(pretrained=False, **kwargs):\n",
    "    model_args = dict(\n",
    "        layers=(12, 12, 20, 4),\n",
    "        embed_dims=(384, 768, 768, 768),\n",
    "        num_heads=(12, 16, 16, 16),\n",
    "        mlp_ratio=4,\n",
    "        stem_hidden_dim=128,\n",
    "        **kwargs\n",
    "    )\n",
    "    model = _create_mivolo(\"mivolo_d5_512\", pretrained=pretrained, **model_args)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c67304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_checkpoint(\n",
    "    model, checkpoint_path, use_ema=True, strict=True, remap=False, filter_keys=None, state_dict_map=None\n",
    "):\n",
    "    if os.path.splitext(checkpoint_path)[-1].lower() in (\".npz\", \".npy\"):\n",
    "        # numpy checkpoint, try to load via model specific load_pretrained fn\n",
    "        if hasattr(model, \"load_pretrained\"):\n",
    "            timm.models._model_builder.load_pretrained(checkpoint_path)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model cannot load numpy checkpoint\")\n",
    "        return\n",
    "    state_dict = load_state_dict(checkpoint_path, use_ema)\n",
    "    if remap:\n",
    "        state_dict = remap_checkpoint(model, state_dict)\n",
    "    if filter_keys:\n",
    "        for sd_key in list(state_dict.keys()):\n",
    "            for filter_key in filter_keys:\n",
    "                if filter_key in sd_key:\n",
    "                    if sd_key in state_dict:\n",
    "                        del state_dict[sd_key]\n",
    "\n",
    "    rep = []\n",
    "    if state_dict_map is not None:\n",
    "        # 'patch_embed.conv1.' : 'patch_embed.conv.'\n",
    "        for state_k in list(state_dict.keys()):\n",
    "            for target_k, target_v in state_dict_map.items():\n",
    "                if target_v in state_k:\n",
    "                    target_name = state_k.replace(target_v, target_k)\n",
    "                    state_dict[target_name] = state_dict[state_k]\n",
    "                    rep.append(state_k)\n",
    "        for r in rep:\n",
    "            if r in state_dict:\n",
    "                del state_dict[r]\n",
    "\n",
    "    incompatible_keys = model.load_state_dict(state_dict, strict=strict if filter_keys is None else False)\n",
    "    return incompatible_keys\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    model_name: str,\n",
    "    pretrained: bool = False,\n",
    "    pretrained_cfg: Optional[Union[str, Dict[str, Any], PretrainedCfg]] = None,\n",
    "    pretrained_cfg_overlay: Optional[Dict[str, Any]] = None,\n",
    "    checkpoint_path: str = \"\",\n",
    "    scriptable: Optional[bool] = None,\n",
    "    exportable: Optional[bool] = None,\n",
    "    no_jit: Optional[bool] = None,\n",
    "    filter_keys=None,\n",
    "    state_dict_map=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Create a model\n",
    "    Lookup model's entrypoint function and pass relevant args to create a new model.\n",
    "    \"\"\"\n",
    "    # Parameters that aren't supported by all models or are intended to only override model defaults if set\n",
    "    # should default to None in command line args/cfg. Remove them if they are present and not set so that\n",
    "    # non-supporting models don't break and default args remain in effect.\n",
    "    kwargs = {k: v for k, v in kwargs.items() if v is not None}\n",
    "\n",
    "    model_source, model_name = parse_model_name(model_name)\n",
    "    if model_source == \"hf-hub\":\n",
    "        assert not pretrained_cfg, \"pretrained_cfg should not be set when sourcing model from Hugging Face Hub.\"\n",
    "        # For model names specified in the form `hf-hub:path/architecture_name@revision`,\n",
    "        # load model weights + pretrained_cfg from Hugging Face hub.\n",
    "        pretrained_cfg, model_name = load_model_config_from_hf(model_name)\n",
    "    else:\n",
    "        model_name, pretrained_tag = split_model_name_tag(model_name)\n",
    "        if not pretrained_cfg:\n",
    "            # a valid pretrained_cfg argument takes priority over tag in model name\n",
    "            pretrained_cfg = pretrained_tag\n",
    "\n",
    "    if not is_model(model_name):\n",
    "        raise RuntimeError(\"Unknown model (%s)\" % model_name)\n",
    "\n",
    "    create_fn = model_entrypoint(model_name)\n",
    "    with set_layer_config(scriptable=scriptable, exportable=exportable, no_jit=no_jit):\n",
    "        model = create_fn(\n",
    "            pretrained=pretrained,\n",
    "            pretrained_cfg=pretrained_cfg,\n",
    "            pretrained_cfg_overlay=pretrained_cfg_overlay,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    if checkpoint_path:\n",
    "        load_checkpoint(model, checkpoint_path, filter_keys=filter_keys, state_dict_map=state_dict_map)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3f9de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_logger = logging.getLogger(\"MiVOLO\")\n",
    "has_compile = hasattr(torch, \"compile\")\n",
    "\n",
    "\n",
    "class Meta:\n",
    "    def __init__(self):\n",
    "        self.min_age = None\n",
    "        self.max_age = None\n",
    "        self.avg_age = None\n",
    "        self.num_classes = None\n",
    "\n",
    "        self.in_chans = 3\n",
    "        self.with_persons_model = False\n",
    "        self.disable_faces = False\n",
    "        self.use_persons = True\n",
    "        self.only_age = False\n",
    "\n",
    "        self.num_classes_gender = 2\n",
    "\n",
    "    def load_from_ckpt(self, ckpt_path: str, disable_faces: bool = False, use_persons: bool = True) -> \"Meta\":\n",
    "\n",
    "        state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        self.min_age = state[\"min_age\"]\n",
    "        self.max_age = state[\"max_age\"]\n",
    "        self.avg_age = state[\"avg_age\"]\n",
    "        self.only_age = state[\"no_gender\"]\n",
    "\n",
    "        only_age = state[\"no_gender\"]\n",
    "\n",
    "        self.disable_faces = disable_faces\n",
    "        if \"with_persons_model\" in state:\n",
    "            self.with_persons_model = state[\"with_persons_model\"]\n",
    "        else:\n",
    "            self.with_persons_model = True if \"patch_embed.conv1.0.weight\" in state[\"state_dict\"] else False\n",
    "\n",
    "        self.num_classes = 1 if only_age else 3\n",
    "        self.in_chans = 3 if not self.with_persons_model else 6\n",
    "        self.use_persons = use_persons and self.with_persons_model\n",
    "\n",
    "        if not self.with_persons_model and self.disable_faces:\n",
    "            raise ValueError(\"You can not use disable-faces for faces-only model\")\n",
    "        if self.with_persons_model and self.disable_faces and not self.use_persons:\n",
    "            raise ValueError(\"You can not disable faces and persons together\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __str__(self):\n",
    "        attrs = vars(self)\n",
    "        attrs.update({\"use_person_crops\": self.use_person_crops, \"use_face_crops\": self.use_face_crops})\n",
    "        return \", \".join(\"%s: %s\" % item for item in attrs.items())\n",
    "\n",
    "    @property\n",
    "    def use_person_crops(self) -> bool:\n",
    "        return self.with_persons_model and self.use_persons\n",
    "\n",
    "    @property\n",
    "    def use_face_crops(self) -> bool:\n",
    "        return not self.disable_faces or not self.with_persons_model\n",
    "\n",
    "\n",
    "class MiVOLO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ckpt_path: str,\n",
    "        device: str = \"cuda\",\n",
    "        half: bool = True,\n",
    "        disable_faces: bool = False,\n",
    "        use_persons: bool = True,\n",
    "        verbose: bool = False,\n",
    "        torchcompile: Optional[str] = None,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.device = torch.device(device)\n",
    "        self.half = half and self.device.type != \"cpu\"\n",
    "\n",
    "        self.meta: Meta = Meta().load_from_ckpt(ckpt_path, disable_faces, use_persons)\n",
    "        if self.verbose:\n",
    "            _logger.info(f\"Model meta:\\n{str(self.meta)}\")\n",
    "\n",
    "        model_name = \"mivolo_d1_224\"\n",
    "        self.model = create_model(\n",
    "            model_name=model_name,\n",
    "            num_classes=self.meta.num_classes,\n",
    "            in_chans=self.meta.in_chans,\n",
    "            pretrained=False,\n",
    "            checkpoint_path=ckpt_path,\n",
    "            filter_keys=[\"fds.\"],\n",
    "        )\n",
    "        self.param_count = sum([m.numel() for m in self.model.parameters()])\n",
    "        _logger.info(f\"Model {model_name} created, param count: {self.param_count}\")\n",
    "\n",
    "        self.data_config = resolve_data_config(\n",
    "            model=self.model,\n",
    "            verbose=verbose,\n",
    "            use_test_size=True,\n",
    "        )\n",
    "        self.data_config[\"crop_pct\"] = 1.0\n",
    "        c, h, w = self.data_config[\"input_size\"]\n",
    "        assert h == w, \"Incorrect data_config\"\n",
    "        self.input_size = w\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        if torchcompile:\n",
    "            assert has_compile, \"A version of torch w/ torch.compile() is required for --compile, possibly a nightly.\"\n",
    "            torch._dynamo.reset()\n",
    "            self.model = torch.compile(self.model, backend=torchcompile)\n",
    "\n",
    "        self.model.eval()\n",
    "        if self.half:\n",
    "            self.model = self.model.half()\n",
    "\n",
    "    def warmup(self, batch_size: int, steps=10):\n",
    "        if self.meta.with_persons_model:\n",
    "            input_size = (6, self.input_size, self.input_size)\n",
    "        else:\n",
    "            input_size = self.data_config[\"input_size\"]\n",
    "\n",
    "        input = torch.randn((batch_size,) + tuple(input_size)).to(self.device)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            out = self.inference(input)  # noqa: F841\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    def inference(self, model_input: torch.tensor) -> torch.tensor:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.half:\n",
    "                model_input = model_input.half()\n",
    "            output = self.model(model_input)\n",
    "        return output\n",
    "\n",
    "    def predict(self, image: np.ndarray, detected_bboxes: PersonAndFaceResult):\n",
    "        if detected_bboxes.n_objects == 0:\n",
    "            return\n",
    "\n",
    "        faces_input, person_input, faces_inds, bodies_inds = self.prepare_crops(image, detected_bboxes)\n",
    "\n",
    "        if self.meta.with_persons_model:\n",
    "            model_input = torch.cat((faces_input, person_input), dim=1)\n",
    "        else:\n",
    "            model_input = faces_input\n",
    "        output = self.inference(model_input)\n",
    "\n",
    "        # write gender and age results into detected_bboxes\n",
    "        self.fill_in_results(output, detected_bboxes, faces_inds, bodies_inds)\n",
    "\n",
    "    def fill_in_results(self, output, detected_bboxes, faces_inds, bodies_inds):\n",
    "        if self.meta.only_age:\n",
    "            age_output = output\n",
    "            gender_probs, gender_indx = None, None\n",
    "        else:\n",
    "            age_output = output[:, 2]\n",
    "            gender_output = output[:, :2].softmax(-1)\n",
    "            gender_probs, gender_indx = gender_output.topk(1)\n",
    "\n",
    "        assert output.shape[0] == len(faces_inds) == len(bodies_inds)\n",
    "\n",
    "        # per face\n",
    "        for index in range(output.shape[0]):\n",
    "            face_ind = faces_inds[index]\n",
    "            body_ind = bodies_inds[index]\n",
    "\n",
    "            # get_age\n",
    "            age = age_output[index].item()\n",
    "            age = age * (self.meta.max_age - self.meta.min_age) + self.meta.avg_age\n",
    "            age = round(age, 2)\n",
    "\n",
    "            detected_bboxes.set_age(face_ind, age)\n",
    "            detected_bboxes.set_age(body_ind, age)\n",
    "\n",
    "            _logger.info(f\"\\tage: {age}\")\n",
    "\n",
    "            if gender_probs is not None:\n",
    "                gender = \"male\" if gender_indx[index].item() == 0 else \"female\"\n",
    "                gender_score = gender_probs[index].item()\n",
    "\n",
    "                _logger.info(f\"\\tgender: {gender} [{int(gender_score * 100)}%]\")\n",
    "\n",
    "                detected_bboxes.set_gender(face_ind, gender, gender_score)\n",
    "                detected_bboxes.set_gender(body_ind, gender, gender_score)\n",
    "\n",
    "    def prepare_crops(self, image: np.ndarray, detected_bboxes: PersonAndFaceResult):\n",
    "\n",
    "        if self.meta.use_person_crops and self.meta.use_face_crops:\n",
    "            detected_bboxes.associate_faces_with_persons()\n",
    "\n",
    "        crops: PersonAndFaceCrops = detected_bboxes.collect_crops(image)\n",
    "        (bodies_inds, bodies_crops), (faces_inds, faces_crops) = crops.get_faces_with_bodies(\n",
    "            self.meta.use_person_crops, self.meta.use_face_crops\n",
    "        )\n",
    "\n",
    "        if not self.meta.use_face_crops:\n",
    "            assert all(f is None for f in faces_crops)\n",
    "\n",
    "        faces_input = prepare_classification_images(\n",
    "            faces_crops, self.input_size, self.data_config[\"mean\"], self.data_config[\"std\"], device=self.device\n",
    "        )\n",
    "\n",
    "        if not self.meta.use_person_crops:\n",
    "            assert all(p is None for p in bodies_crops)\n",
    "\n",
    "        person_input = prepare_classification_images(\n",
    "            bodies_crops, self.input_size, self.data_config[\"mean\"], self.data_config[\"std\"], device=self.device\n",
    "        )\n",
    "\n",
    "        _logger.info(\n",
    "            f\"faces_input: {faces_input.shape if faces_input is not None else None}, \"\n",
    "            f\"person_input: {person_input.shape if person_input is not None else None}\"\n",
    "        )\n",
    "\n",
    "        return faces_input, person_input, faces_inds, bodies_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fc16d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.unsetenv(\"CUBLAS_WORKSPACE_CONFIG\")\n",
    "\n",
    "\n",
    "class Detector:\n",
    "    def __init__(\n",
    "        self,\n",
    "        weights: str,\n",
    "        device: str = \"cuda\",\n",
    "        half: bool = True,\n",
    "        verbose: bool = False,\n",
    "        conf_thresh: float = 0.4,\n",
    "        iou_thresh: float = 0.7,\n",
    "    ):\n",
    "        self.yolo = YOLO(weights)\n",
    "        self.yolo.fuse()\n",
    "\n",
    "        self.device = torch.device(device)\n",
    "        self.half = half and self.device.type != \"cpu\"\n",
    "\n",
    "        if self.half:\n",
    "            self.yolo.model = self.yolo.model.half()\n",
    "\n",
    "        self.detector_names: Dict[int, str] = self.yolo.model.names\n",
    "\n",
    "        # init yolo.predictor\n",
    "        self.detector_kwargs = {\"conf\": conf_thresh, \"iou\": iou_thresh, \"half\": self.half, \"verbose\": verbose}\n",
    "        # self.yolo.predict(**self.detector_kwargs)\n",
    "\n",
    "    def predict(self, image: Union[np.ndarray, str, \"PIL.Image\"]) -> PersonAndFaceResult:\n",
    "        results: Results = self.yolo.predict(image, **self.detector_kwargs)[0]\n",
    "        return PersonAndFaceResult(results)\n",
    "\n",
    "    def track(self, image: Union[np.ndarray, str, \"PIL.Image\"]) -> PersonAndFaceResult:\n",
    "        results: Results = self.yolo.track(image, persist=True, **self.detector_kwargs)[0]\n",
    "        return PersonAndFaceResult(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2486cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because of ultralytics bug it is important to unset CUBLAS_WORKSPACE_CONFIG after the module importing\n",
    "os.unsetenv(\"CUBLAS_WORKSPACE_CONFIG\")\n",
    "\n",
    "AGE_GENDER_TYPE = Tuple[float, str]\n",
    "\n",
    "\n",
    "class PersonAndFaceCrops:\n",
    "    def __init__(self):\n",
    "        # int: index of person along results\n",
    "        self.crops_persons: Dict[int, np.ndarray] = {}\n",
    "\n",
    "        # int: index of face along results\n",
    "        self.crops_faces: Dict[int, np.ndarray] = {}\n",
    "\n",
    "        # int: index of face along results\n",
    "        self.crops_faces_wo_body: Dict[int, np.ndarray] = {}\n",
    "\n",
    "        # int: index of person along results\n",
    "        self.crops_persons_wo_face: Dict[int, np.ndarray] = {}\n",
    "\n",
    "    def _add_to_output(\n",
    "        self, crops: Dict[int, np.ndarray], out_crops: List[np.ndarray], out_crop_inds: List[Optional[int]]\n",
    "    ):\n",
    "        inds_to_add = list(crops.keys())\n",
    "        crops_to_add = list(crops.values())\n",
    "        out_crops.extend(crops_to_add)\n",
    "        out_crop_inds.extend(inds_to_add)\n",
    "\n",
    "    def _get_all_faces(\n",
    "        self, use_persons: bool, use_faces: bool\n",
    "    ) -> Tuple[List[Optional[int]], List[Optional[np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "            if use_persons and use_faces\n",
    "                faces: faces_with_bodies + faces_without_bodies + [None] * len(crops_persons_wo_face)\n",
    "            if use_persons and not use_faces\n",
    "                faces: [None] * n_persons\n",
    "            if not use_persons and use_faces:\n",
    "                faces: faces_with_bodies + faces_without_bodies\n",
    "        \"\"\"\n",
    "\n",
    "        def add_none_to_output(faces_inds, faces_crops, num):\n",
    "            faces_inds.extend([None for _ in range(num)])\n",
    "            faces_crops.extend([None for _ in range(num)])\n",
    "\n",
    "        faces_inds: List[Optional[int]] = []\n",
    "        faces_crops: List[Optional[np.ndarray]] = []\n",
    "\n",
    "        if not use_faces:\n",
    "            add_none_to_output(faces_inds, faces_crops, len(self.crops_persons) + len(self.crops_persons_wo_face))\n",
    "            return faces_inds, faces_crops\n",
    "\n",
    "        self._add_to_output(self.crops_faces, faces_crops, faces_inds)\n",
    "        self._add_to_output(self.crops_faces_wo_body, faces_crops, faces_inds)\n",
    "\n",
    "        if use_persons:\n",
    "            add_none_to_output(faces_inds, faces_crops, len(self.crops_persons_wo_face))\n",
    "\n",
    "        return faces_inds, faces_crops\n",
    "\n",
    "    def _get_all_bodies(\n",
    "        self, use_persons: bool, use_faces: bool\n",
    "    ) -> Tuple[List[Optional[int]], List[Optional[np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "            if use_persons and use_faces\n",
    "                persons: bodies_with_faces + [None] * len(faces_without_bodies) + bodies_without_faces\n",
    "            if use_persons and not use_faces\n",
    "                persons: bodies_with_faces + bodies_without_faces\n",
    "            if not use_persons and use_faces\n",
    "                persons: [None] * n_faces\n",
    "        \"\"\"\n",
    "\n",
    "        def add_none_to_output(bodies_inds, bodies_crops, num):\n",
    "            bodies_inds.extend([None for _ in range(num)])\n",
    "            bodies_crops.extend([None for _ in range(num)])\n",
    "\n",
    "        bodies_inds: List[Optional[int]] = []\n",
    "        bodies_crops: List[Optional[np.ndarray]] = []\n",
    "\n",
    "        if not use_persons:\n",
    "            add_none_to_output(bodies_inds, bodies_crops, len(self.crops_faces) + len(self.crops_faces_wo_body))\n",
    "            return bodies_inds, bodies_crops\n",
    "\n",
    "        self._add_to_output(self.crops_persons, bodies_crops, bodies_inds)\n",
    "        if use_faces:\n",
    "            add_none_to_output(bodies_inds, bodies_crops, len(self.crops_faces_wo_body))\n",
    "\n",
    "        self._add_to_output(self.crops_persons_wo_face, bodies_crops, bodies_inds)\n",
    "\n",
    "        return bodies_inds, bodies_crops\n",
    "\n",
    "    def get_faces_with_bodies(self, use_persons: bool, use_faces: bool):\n",
    "        \"\"\"\n",
    "        Return\n",
    "            faces: faces_with_bodies, faces_without_bodies, [None] * len(crops_persons_wo_face)\n",
    "            persons: bodies_with_faces, [None] * len(faces_without_bodies), bodies_without_faces\n",
    "        \"\"\"\n",
    "\n",
    "        bodies_inds, bodies_crops = self._get_all_bodies(use_persons, use_faces)\n",
    "        faces_inds, faces_crops = self._get_all_faces(use_persons, use_faces)\n",
    "\n",
    "        return (bodies_inds, bodies_crops), (faces_inds, faces_crops)\n",
    "\n",
    "    def save(self, out_dir=\"output\"):\n",
    "        ind = 0\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        for crops in [self.crops_persons, self.crops_faces, self.crops_faces_wo_body, self.crops_persons_wo_face]:\n",
    "            for crop in crops.values():\n",
    "                if crop is None:\n",
    "                    continue\n",
    "                out_name = os.path.join(out_dir, f\"{ind}_crop.jpg\")\n",
    "                cv2.imwrite(out_name, crop)\n",
    "                ind += 1\n",
    "\n",
    "class PersonAndFaceResult:\n",
    "    def __init__(self, results: Results):\n",
    "\n",
    "        self.yolo_results = results\n",
    "        names = set(results.names.values())\n",
    "        assert \"person\" in names and \"face\" in names\n",
    "\n",
    "        # initially no faces and persons are associated to each other\n",
    "        self.face_to_person_map: Dict[int, Optional[int]] = {ind: None for ind in self.get_bboxes_inds(\"face\")}\n",
    "        self.unassigned_persons_inds: List[int] = self.get_bboxes_inds(\"person\")\n",
    "        n_objects = len(self.yolo_results.boxes)\n",
    "        self.ages: List[Optional[float]] = [None for _ in range(n_objects)]\n",
    "        self.genders: List[Optional[str]] = [None for _ in range(n_objects)]\n",
    "        self.gender_scores: List[Optional[float]] = [None for _ in range(n_objects)]\n",
    "\n",
    "    @property\n",
    "    def n_objects(self) -> int:\n",
    "        return len(self.yolo_results.boxes)\n",
    "\n",
    "    def get_bboxes_inds(self, category: str) -> List[int]:\n",
    "        bboxes: List[int] = []\n",
    "        for ind, det in enumerate(self.yolo_results.boxes):\n",
    "            name = self.yolo_results.names[int(det.cls)]\n",
    "            if name == category:\n",
    "                bboxes.append(ind)\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "    def get_distance_to_center(self, bbox_ind: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate euclidian distance between bbox center and image center.\n",
    "        \"\"\"\n",
    "        im_h, im_w = self.yolo_results[bbox_ind].orig_shape\n",
    "        x1, y1, x2, y2 = self.get_bbox_by_ind(bbox_ind).cpu().numpy()\n",
    "        center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        dist = math.dist([center_x, center_y], [im_w / 2, im_h / 2])\n",
    "        return dist\n",
    "\n",
    "    def plot(\n",
    "        self,\n",
    "        conf=False,\n",
    "        line_width=None,\n",
    "        font_size=None,\n",
    "        font=\"Arial.ttf\",\n",
    "        pil=False,\n",
    "        img=None,\n",
    "        labels=True,\n",
    "        boxes=True,\n",
    "        probs=True,\n",
    "        ages=True,\n",
    "        genders=True,\n",
    "        gender_probs=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots the detection results on an input RGB image. Accepts a numpy array (cv2) or a PIL Image.\n",
    "        Args:\n",
    "            conf (bool): Whether to plot the detection confidence score.\n",
    "            line_width (float, optional): The line width of the bounding boxes. If None, it is scaled to the image size.\n",
    "            font_size (float, optional): The font size of the text. If None, it is scaled to the image size.\n",
    "            font (str): The font to use for the text.\n",
    "            pil (bool): Whether to return the image as a PIL Image.\n",
    "            img (numpy.ndarray): Plot to another image. if not, plot to original image.\n",
    "            labels (bool): Whether to plot the label of bounding boxes.\n",
    "            boxes (bool): Whether to plot the bounding boxes.\n",
    "            probs (bool): Whether to plot classification probability\n",
    "            ages (bool): Whether to plot the age of bounding boxes.\n",
    "            genders (bool): Whether to plot the genders of bounding boxes.\n",
    "            gender_probs (bool): Whether to plot gender classification probability\n",
    "        Returns:\n",
    "            (numpy.ndarray): A numpy array of the annotated image.\n",
    "        \"\"\"\n",
    "\n",
    "        # return self.yolo_results.plot()\n",
    "        colors_by_ind = {}\n",
    "        for face_ind, person_ind in self.face_to_person_map.items():\n",
    "            if person_ind is not None:\n",
    "                #colors_by_ind[face_ind] = face_ind + 2\n",
    "                colors_by_ind[person_ind] = face_ind + 2\n",
    "            else:\n",
    "                colors_by_ind[face_ind] = 0\n",
    "        for person_ind in self.unassigned_persons_inds:\n",
    "            colors_by_ind[person_ind] = 1\n",
    "\n",
    "        names = self.yolo_results.names\n",
    "        annotator = Annotator(\n",
    "            deepcopy(self.yolo_results.orig_img if img is None else img),\n",
    "            line_width,\n",
    "            font_size,\n",
    "            font,\n",
    "            pil,\n",
    "            example=names,\n",
    "        )\n",
    "        pred_boxes, show_boxes = self.yolo_results.boxes, boxes\n",
    "        pred_probs, show_probs = self.yolo_results.probs, probs\n",
    "\n",
    "        if pred_boxes and show_boxes:\n",
    "            for bb_ind, (d, age, gender, gender_score) in enumerate(\n",
    "                zip(pred_boxes, self.ages, self.genders, self.gender_scores)\n",
    "            ):\n",
    "                c, conf, guid = int(d.cls), float(d.conf) if conf else None, None if d.id is None else int(d.id.item())\n",
    "                name = (\"\" if guid is None else f\"id:{guid} \") + names[c]\n",
    "                label = (f\"{name} {conf:.2f}\" if conf else name) if labels else None\n",
    "                if ages and age is not None:\n",
    "                    label += f\" {age:.1f}\"\n",
    "                if genders and gender is not None:\n",
    "                    label += f\" {'F' if gender == 'female' else 'M'}\"\n",
    "                if gender_probs and gender_score is not None:\n",
    "                    label += f\" ({gender_score:.1f})\"\n",
    "                annotator.box_label(d.xyxy.squeeze(), label, color=colors(colors_by_ind[bb_ind], True))\n",
    "\n",
    "        if pred_probs is not None and show_probs:\n",
    "            text = f\"{', '.join(f'{names[j] if names else j} {pred_probs.data[j]:.2f}' for j in pred_probs.top5)}, \"\n",
    "            annotator.text((32, 32), text, txt_color=(255, 255, 255))  # TODO: allow setting colors\n",
    "\n",
    "        return annotator.result()\n",
    "\n",
    "    def set_tracked_age_gender(self, tracked_objects: Dict[int, List[AGE_GENDER_TYPE]]):\n",
    "        \"\"\"\n",
    "        Update age and gender for objects based on history from tracked_objects.\n",
    "        Args:\n",
    "            tracked_objects (dict[int, list[AGE_GENDER_TYPE]]): info about tracked objects by guid\n",
    "        \"\"\"\n",
    "\n",
    "        for face_ind, person_ind in self.face_to_person_map.items():\n",
    "            pguid = self._get_id_by_ind(person_ind)\n",
    "            fguid = self._get_id_by_ind(face_ind)\n",
    "\n",
    "            if fguid == -1 and pguid == -1:\n",
    "                # YOLO might not assign ids for some objects in some cases:\n",
    "                # https://github.com/ultralytics/ultralytics/issues/3830\n",
    "                continue\n",
    "            age, gender = self._gather_tracking_result(tracked_objects, fguid, pguid)\n",
    "            if age is None or gender is None:\n",
    "                continue\n",
    "            self.set_age(face_ind, age)\n",
    "            self.set_gender(face_ind, gender, 1.0)\n",
    "            if pguid != -1:\n",
    "                self.set_gender(person_ind, gender, 1.0)\n",
    "                self.set_age(person_ind, age)\n",
    "\n",
    "        for person_ind in self.unassigned_persons_inds:\n",
    "            pid = self._get_id_by_ind(person_ind)\n",
    "            if pid == -1:\n",
    "                continue\n",
    "            age, gender = self._gather_tracking_result(tracked_objects, -1, pid)\n",
    "            if age is None or gender is None:\n",
    "                continue\n",
    "            self.set_gender(person_ind, gender, 1.0)\n",
    "            self.set_age(person_ind, age)\n",
    "\n",
    "    def _get_id_by_ind(self, ind: Optional[int] = None) -> int:\n",
    "        if ind is None:\n",
    "            return -1\n",
    "        obj_id = self.yolo_results.boxes[ind].id\n",
    "        if obj_id is None:\n",
    "            return -1\n",
    "        return obj_id.item()\n",
    "\n",
    "    def get_bbox_by_ind(self, ind: int, im_h: int = None, im_w: int = None) -> torch.tensor:\n",
    "        bb = self.yolo_results.boxes[ind].xyxy.squeeze().type(torch.int32)\n",
    "        if im_h is not None and im_w is not None:\n",
    "            bb[0] = torch.clamp(bb[0], min=0, max=im_w - 1)\n",
    "            bb[1] = torch.clamp(bb[1], min=0, max=im_h - 1)\n",
    "            bb[2] = torch.clamp(bb[2], min=0, max=im_w - 1)\n",
    "            bb[3] = torch.clamp(bb[3], min=0, max=im_h - 1)\n",
    "        return bb\n",
    "\n",
    "    def set_age(self, ind: Optional[int], age: float):\n",
    "        if ind is not None:\n",
    "            self.ages[ind] = age\n",
    "\n",
    "    def set_gender(self, ind: Optional[int], gender: str, gender_score: float):\n",
    "        if ind is not None:\n",
    "            self.genders[ind] = gender\n",
    "            self.gender_scores[ind] = gender_score\n",
    "\n",
    "    @staticmethod\n",
    "    def _gather_tracking_result(\n",
    "        tracked_objects: Dict[int, List[AGE_GENDER_TYPE]],\n",
    "        fguid: int = -1,\n",
    "        pguid: int = -1,\n",
    "        minimum_sample_size: int = 10,\n",
    "    ) -> AGE_GENDER_TYPE:\n",
    "\n",
    "        assert fguid != -1 or pguid != -1, \"Incorrect tracking behaviour\"\n",
    "\n",
    "        face_ages = [r[0] for r in tracked_objects[fguid] if r[0] is not None] if fguid in tracked_objects else []\n",
    "        face_genders = [r[1] for r in tracked_objects[fguid] if r[1] is not None] if fguid in tracked_objects else []\n",
    "        person_ages = [r[0] for r in tracked_objects[pguid] if r[0] is not None] if pguid in tracked_objects else []\n",
    "        person_genders = [r[1] for r in tracked_objects[pguid] if r[1] is not None] if pguid in tracked_objects else []\n",
    "\n",
    "        if not face_ages and not person_ages: # both empty\n",
    "            return None, None\n",
    "\n",
    "        # You can play here with different aggregation strategies\n",
    "        # Face ages - predictions based on face or face + person, depends on history of object\n",
    "        # Person ages - predictions based on person or face + person, depends on history of object\n",
    "\n",
    "        if len(person_ages + face_ages) >= minimum_sample_size:\n",
    "            age = aggregate_votes_winsorized(person_ages + face_ages)\n",
    "        else:\n",
    "            face_age = np.mean(face_ages) if face_ages else None\n",
    "            person_age = np.mean(person_ages) if person_ages else None\n",
    "            if face_age is None:\n",
    "                face_age = person_age\n",
    "            if person_age is None:\n",
    "                person_age = face_age\n",
    "            age = (face_age + person_age) / 2.0\n",
    "\n",
    "        genders = face_genders + person_genders\n",
    "        assert len(genders) > 0\n",
    "        # take mode of genders\n",
    "        gender = max(set(genders), key=genders.count)\n",
    "\n",
    "        return age, gender\n",
    "\n",
    "    def get_results_for_tracking(self) -> Tuple[Dict[int, AGE_GENDER_TYPE], Dict[int, AGE_GENDER_TYPE]]:\n",
    "        \"\"\"\n",
    "        Get objects from current frame\n",
    "        \"\"\"\n",
    "        persons: Dict[int, AGE_GENDER_TYPE] = {}\n",
    "        faces: Dict[int, AGE_GENDER_TYPE] = {}\n",
    "\n",
    "        names = self.yolo_results.names\n",
    "        pred_boxes = self.yolo_results.boxes\n",
    "        for _, (det, age, gender, _) in enumerate(zip(pred_boxes, self.ages, self.genders, self.gender_scores)):\n",
    "            if det.id is None:\n",
    "                continue\n",
    "            cat_id, _, guid = int(det.cls), float(det.conf), int(det.id.item())\n",
    "            name = names[cat_id]\n",
    "            if name == \"person\":\n",
    "                persons[guid] = (age, gender)\n",
    "            elif name == \"face\":\n",
    "                faces[guid] = (age, gender)\n",
    "\n",
    "        return persons, faces\n",
    "\n",
    "    def associate_faces_with_persons(self):\n",
    "        face_bboxes_inds: List[int] = self.get_bboxes_inds(\"face\")\n",
    "        person_bboxes_inds: List[int] = self.get_bboxes_inds(\"person\")\n",
    "\n",
    "        face_bboxes: List[torch.tensor] = [self.get_bbox_by_ind(ind) for ind in face_bboxes_inds]\n",
    "        person_bboxes: List[torch.tensor] = [self.get_bbox_by_ind(ind) for ind in person_bboxes_inds]\n",
    "\n",
    "        self.face_to_person_map = {ind: None for ind in face_bboxes_inds}\n",
    "        assigned_faces, unassigned_persons_inds = assign_faces(person_bboxes, face_bboxes)\n",
    "\n",
    "        for face_ind, person_ind in enumerate(assigned_faces):\n",
    "            face_ind = face_bboxes_inds[face_ind]\n",
    "            person_ind = person_bboxes_inds[person_ind] if person_ind is not None else None\n",
    "            self.face_to_person_map[face_ind] = person_ind\n",
    "\n",
    "        self.unassigned_persons_inds = [person_bboxes_inds[person_ind] for person_ind in unassigned_persons_inds]\n",
    "\n",
    "    def crop_object(\n",
    "        self, full_image: np.ndarray, ind: int, cut_other_classes: Optional[List[str]] = None\n",
    "    ) -> Optional[np.ndarray]:\n",
    "\n",
    "        IOU_THRESH = 0.000001\n",
    "        MIN_PERSON_CROP_AFTERCUT_RATIO = 0.4\n",
    "        CROP_ROUND_RATE = 0.3\n",
    "        MIN_PERSON_SIZE = 50\n",
    "\n",
    "        obj_bbox = self.get_bbox_by_ind(ind, *full_image.shape[:2])\n",
    "        x1, y1, x2, y2 = obj_bbox\n",
    "        cur_cat = self.yolo_results.names[int(self.yolo_results.boxes[ind].cls)]\n",
    "        # get crop of face or person\n",
    "        obj_image = full_image[y1:y2, x1:x2].copy()\n",
    "        crop_h, crop_w = obj_image.shape[:2]\n",
    "\n",
    "        if cur_cat == \"person\" and (crop_h < MIN_PERSON_SIZE or crop_w < MIN_PERSON_SIZE):\n",
    "            return None\n",
    "\n",
    "        if not cut_other_classes:\n",
    "            return obj_image\n",
    "\n",
    "        # calc iou between obj_bbox and other bboxes\n",
    "        other_bboxes: List[torch.tensor] = [\n",
    "            self.get_bbox_by_ind(other_ind, *full_image.shape[:2]) for other_ind in range(len(self.yolo_results.boxes))\n",
    "        ]\n",
    "\n",
    "        iou_matrix = box_iou(torch.stack([obj_bbox]), torch.stack(other_bboxes)).cpu().numpy()[0]\n",
    "\n",
    "        # cut out other objects in case of intersection\n",
    "        for other_ind, (det, iou) in enumerate(zip(self.yolo_results.boxes, iou_matrix)):\n",
    "            other_cat = self.yolo_results.names[int(det.cls)]\n",
    "            if ind == other_ind or iou < IOU_THRESH or other_cat not in cut_other_classes:\n",
    "                continue\n",
    "            o_x1, o_y1, o_x2, o_y2 = det.xyxy.squeeze().type(torch.int32)\n",
    "\n",
    "            # remap current_person_bbox to reference_person_bbox coordinates\n",
    "            o_x1 = max(o_x1 - x1, 0)\n",
    "            o_y1 = max(o_y1 - y1, 0)\n",
    "            o_x2 = min(o_x2 - x1, crop_w)\n",
    "            o_y2 = min(o_y2 - y1, crop_h)\n",
    "\n",
    "            if other_cat != \"face\":\n",
    "                if (o_y1 / crop_h) < CROP_ROUND_RATE:\n",
    "                    o_y1 = 0\n",
    "                if ((crop_h - o_y2) / crop_h) < CROP_ROUND_RATE:\n",
    "                    o_y2 = crop_h\n",
    "                if (o_x1 / crop_w) < CROP_ROUND_RATE:\n",
    "                    o_x1 = 0\n",
    "                if ((crop_w - o_x2) / crop_w) < CROP_ROUND_RATE:\n",
    "                    o_x2 = crop_w\n",
    "\n",
    "            obj_image[o_y1:o_y2, o_x1:o_x2] = 0\n",
    "\n",
    "        obj_image, remain_ratio = cropout_black_parts(obj_image, CROP_ROUND_RATE)\n",
    "        if remain_ratio < MIN_PERSON_CROP_AFTERCUT_RATIO:\n",
    "            return None\n",
    "\n",
    "        return obj_image\n",
    "\n",
    "    def collect_crops(self, image) -> PersonAndFaceCrops:\n",
    "\n",
    "        crops_data = PersonAndFaceCrops()\n",
    "        for face_ind, person_ind in self.face_to_person_map.items():\n",
    "            face_image = self.crop_object(image, face_ind, cut_other_classes=[])\n",
    "\n",
    "            if person_ind is None:\n",
    "                crops_data.crops_faces_wo_body[face_ind] = face_image\n",
    "                continue\n",
    "\n",
    "            person_image = self.crop_object(image, person_ind, cut_other_classes=[\"face\", \"person\"])\n",
    "\n",
    "            crops_data.crops_faces[face_ind] = face_image\n",
    "            crops_data.crops_persons[person_ind] = person_image\n",
    "\n",
    "        for person_ind in self.unassigned_persons_inds:\n",
    "            person_image = self.crop_object(image, person_ind, cut_other_classes=[\"face\", \"person\"])\n",
    "            crops_data.crops_persons_wo_face[person_ind] = person_image\n",
    "\n",
    "        # uncomment to save preprocessed crops\n",
    "        # crops_data.save()\n",
    "        return crops_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b361f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, config, verbose: bool = False):\n",
    "        self.detector = Detector(config.detector_weights, config.device, verbose=verbose)\n",
    "        self.age_gender_model = MiVOLO(\n",
    "            config.checkpoint,\n",
    "            config.device,\n",
    "            half=True,\n",
    "            use_persons=config.with_persons,\n",
    "            disable_faces=config.disable_faces,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.draw = config.draw\n",
    "    def recognize(self, image: np.ndarray) -> Tuple[PersonAndFaceResult, Optional[np.ndarray]]:\n",
    "        detected_objects: PersonAndFaceResult = self.detector.predict(image)\n",
    "        self.age_gender_model.predict(image, detected_objects)\n",
    "        out_im = None\n",
    "        if self.draw:\n",
    "            # plot results on image\n",
    "            out_im = detected_objects.plot()\n",
    "        return detected_objects, out_im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0b03321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually create an args object with the required attributes\n",
    "class Args:\n",
    "    input = \"face.jfif\"\n",
    "    output = \"outputs\"\n",
    "    detector_weights = \"models/yolov8x_person_face.pt\"\n",
    "    checkpoint = \"models/mivolo_imbd.pth.tar\"\n",
    "    device = \"cpu\" \n",
    "    with_persons = True\n",
    "    disable_faces = True\n",
    "    draw = True  # Set this to True if you want to draw on the image\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5cd4c1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 268 layers, 68125494 parameters, 0 gradients\n",
      "\n",
      "0: 384x640 1 person, 1 face, 2279.0ms\n",
      "Speed: 6.0ms preprocess, 2279.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(args, verbose=True)\n",
    "img = cv2.imread(\"child.jfif\")\n",
    "detected_objects, out_im = predictor.recognize(img)\n",
    "# Display the output image\n",
    "cv2.imshow(\"Output Image\", out_im)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47b1e8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PersonAndFaceResult at 0x1a8a0927310>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc3aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
